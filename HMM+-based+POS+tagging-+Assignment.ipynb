{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMMs and Viterbi algorithm for POS tagging Assignment\n",
    "## POS tagging using modified Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "#### HMMs and Viterbi algorithm for POS tagging\n",
    " \n",
    "* We have learnt to build HMM-based POS tagger and implement the Viterbi algorithm using the Penn Treebank training corpus. The vanilla Viterbi algorithm we had written had resulted in ~87% accuracy. The approx. 13% loss of accuracy was majorly due to the fact that when the algorithm encountered an unknown word (i.e. not present in the training set, such as 'Twitter'), it assigned an incorrect tag arbitrarily. This is because, for unknown words, the emission probabilities for all candidate tags are 0, so the algorithm arbitrarily chooses (the first) tag.\n",
    "\n",
    "\n",
    "* In this assignment, we need to modify the Viterbi algorithm to solve the problem of unknown words using at least two techniques. Below steps are included\n",
    "    * Data Preparation - Exploratory analysis on tagged corpus\n",
    "    * Viterbi Plain Vanilla Algorithm\n",
    "    * Viterbi Modification - Technique 1\n",
    "        * Using Transition probability in case of unknown words\n",
    "    * Viterbi Modification - Technique 2\n",
    "        * Rule based tagger in case of unknown words and also by using transition probability in algorithm in case if rule based tagger returns default tag\n",
    "    * The modified Viterbi POS tagger algorithm is evaluated on the validation and the given test datasets \n",
    "    * Measuring Time taken and accuracy for comparison\n",
    "    * Based on the results, one modified algorithm will be selected which is final algorithm.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "* For this assignment,Using the Treebank dataset of NLTK with the 'universal' tagset. The Universal tagset of NLTK comprises only 12 coarse tag classes as follows: Verb, Noun, Pronouns, Adjectives, Adverbs, Adpositions, Conjunctions, Determiners, Cardinal Numbers, Particles, Other/ Foreign words, Punctuations.\n",
    "\n",
    "### Exploratory analysis on a tagged corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imorting Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Model Selection - Train and test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "#NTLK Libraries\n",
    "import nltk, re, pprint\n",
    "from nltk.tokenize import word_tokenize\n",
    "import requests\n",
    "# Pretty Print # To measure Time taken\n",
    "import pprint, time\n",
    "# Random Numbers\n",
    "import random\n",
    "# visualizations in Python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Treebank Sentences\n",
    "\n",
    "Using Penn Treebank dataset. It is comprised of serveral corpus which are tagged already and we are taking one of those corpus and train HMM based on this and also test HMM based on what we trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "#Taking Treebank from nltk.corpus\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')], [('Rudolph', 'NOUN'), ('Agnew', 'NOUN'), (',', '.'), ('55', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), ('and', 'CONJ'), ('former', 'ADJ'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Consolidated', 'NOUN'), ('Gold', 'NOUN'), ('Fields', 'NOUN'), ('PLC', 'NOUN'), (',', '.'), ('was', 'VERB'), ('named', 'VERB'), ('*-1', 'X'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('British', 'ADJ'), ('industrial', 'ADJ'), ('conglomerate', 'NOUN'), ('.', '.')], [('A', 'DET'), ('form', 'NOUN'), ('of', 'ADP'), ('asbestos', 'NOUN'), ('once', 'ADV'), ('used', 'VERB'), ('*', 'X'), ('*', 'X'), ('to', 'PRT'), ('make', 'VERB'), ('Kent', 'NOUN'), ('cigarette', 'NOUN'), ('filters', 'NOUN'), ('has', 'VERB'), ('caused', 'VERB'), ('a', 'DET'), ('high', 'ADJ'), ('percentage', 'NOUN'), ('of', 'ADP'), ('cancer', 'NOUN'), ('deaths', 'NOUN'), ('among', 'ADP'), ('a', 'DET'), ('group', 'NOUN'), ('of', 'ADP'), ('workers', 'NOUN'), ('exposed', 'VERB'), ('*', 'X'), ('to', 'PRT'), ('it', 'PRON'), ('more', 'ADV'), ('than', 'ADP'), ('30', 'NUM'), ('years', 'NOUN'), ('ago', 'ADP'), (',', '.'), ('researchers', 'NOUN'), ('reported', 'VERB'), ('0', 'X'), ('*T*-1', 'X'), ('.', '.')], [('The', 'DET'), ('asbestos', 'NOUN'), ('fiber', 'NOUN'), (',', '.'), ('crocidolite', 'NOUN'), (',', '.'), ('is', 'VERB'), ('unusually', 'ADV'), ('resilient', 'ADJ'), ('once', 'ADP'), ('it', 'PRON'), ('enters', 'VERB'), ('the', 'DET'), ('lungs', 'NOUN'), (',', '.'), ('with', 'ADP'), ('even', 'ADV'), ('brief', 'ADJ'), ('exposures', 'NOUN'), ('to', 'PRT'), ('it', 'PRON'), ('causing', 'VERB'), ('symptoms', 'NOUN'), ('that', 'DET'), ('*T*-1', 'X'), ('show', 'VERB'), ('up', 'PRT'), ('decades', 'NOUN'), ('later', 'ADJ'), (',', '.'), ('researchers', 'NOUN'), ('said', 'VERB'), ('0', 'X'), ('*T*-2', 'X'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "#Printing First few tagged sentences\n",
    "print(nltk_data[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crisp:\n",
    "* Tagged sentences comprised of every word in a sentence associated with parts of tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Treebank dataset into train and validation sets\n",
    "    * As mentioned in the goals, Using sample size of 95:5 for training: validation sets\n",
    "    * Validation size small, else the algorithm will need a very high amount of runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in to 95:5 for training: validation sets\n",
    "training_set,validation_set=train_test_split(nltk_data,test_size=0.05,train_size=0.95,random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences in the training_set   :  3718\n",
      "Number if sentences in the Validation_set :  196\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Sentences in the training_set   : \",len(training_set))\n",
    "print(\"Number if sentences in the Validation_set : \",len(validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('*', 'X'), ('Encouraging', 'VERB'), ('long-term', 'ADJ'), ('investing', 'NOUN'), ('.', '.')], [('Because', 'ADP'), ('of', 'ADP'), ('the', 'DET'), ('rulings', 'NOUN'), (',', '.'), ('the', 'DET'), ('Commerce', 'NOUN'), ('Department', 'NOUN'), ('will', 'VERB'), ('continue', 'VERB'), ('*-1', 'X'), ('to', 'PRT'), ('investigate', 'VERB'), ('complaints', 'NOUN'), ('*ICH*-2', 'X'), ('by', 'ADP'), ('U.S.', 'NOUN'), ('sweater', 'NOUN'), ('makers', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('imports', 'NOUN'), ('are', 'VERB'), ('reaching', 'VERB'), ('the', 'DET'), ('U.S.', 'NOUN'), ('at', 'ADP'), ('unfairly', 'ADV'), ('low', 'ADJ'), ('prices', 'NOUN'), ('in', 'ADP'), ('violation', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('U.S.', 'NOUN'), ('anti-dumping', 'ADJ'), ('act', 'NOUN'), ('.', '.')], [('What', 'PRON'), ('she', 'PRON'), ('did', 'VERB'), ('*T*-97', 'X'), ('was', 'VERB'), ('like', 'ADP'), ('*', 'X'), ('taking', 'VERB'), ('the', 'DET'), ('law', 'NOUN'), ('into', 'ADP'), ('your', 'PRON'), ('own', 'ADJ'), ('hands', 'NOUN'), ('.', '.'), (\"''\", '.')], [('Los', 'NOUN'), ('Angeles', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('sprawling', 'ADJ'), (',', '.'), ('balkanized', 'ADJ'), ('newspaper', 'NOUN'), ('market', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('advertisers', 'NOUN'), ('seemed', 'VERB'), ('*-1', 'X'), ('to', 'PRT'), ('feel', 'VERB'), ('0', 'X'), ('they', 'PRON'), ('could', 'VERB'), ('buy', 'VERB'), ('space', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('mammoth', 'ADJ'), ('Times', 'NOUN'), (',', '.'), ('then', 'ADV'), ('target', 'VERB'), ('a', 'DET'), ('particular', 'ADJ'), ('area', 'NOUN'), ('with', 'ADP'), ('one', 'NUM'), ('of', 'ADP'), ('the', 'DET'), ('regional', 'ADJ'), ('dailies', 'NOUN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "#Printing First few tagged sentences in training data set\n",
    "print(training_set[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the set of words in training set\n",
    "* In training set, we are taking each sentence and each word from it and adding it to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of list which contains words of the sentences:: 95547\n"
     ]
    }
   ],
   "source": [
    "# Getting list of tagged words\n",
    "train_tagged_words = [tup for sent in training_set for tup in sent]\n",
    "print(\"Length of list which contains words of the sentences::\",len(train_tagged_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique Vocabulary Terms\n",
    "* Tokens - In the training set taking first part of the training word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reliance',\n",
       " 'confirmed',\n",
       " 'the',\n",
       " 'filing',\n",
       " 'but',\n",
       " 'would',\n",
       " \"n't\",\n",
       " 'elaborate',\n",
       " '.',\n",
       " '*',\n",
       " 'Encouraging',\n",
       " 'long-term',\n",
       " 'investing',\n",
       " '.',\n",
       " 'Because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'rulings',\n",
       " ',',\n",
       " 'the']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens - In the training set taking first part of the training word\n",
    "tokens = [pair[0] for pair in train_tagged_words]\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the tokens - Unique Vocabulary terms:: 12100\n"
     ]
    }
   ],
   "source": [
    "# vocabulary\n",
    "Vocab = set(tokens)\n",
    "print(\"Length of the tokens - Unique Vocabulary terms::\",len(Vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of tags and its distribution\n",
    "* In the training set taking first part of the training word which is the tag and putting it in to the set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 different tags used in the training data set\n"
     ]
    }
   ],
   "source": [
    "# Number of tags and its distribution\n",
    "Tags_D = set([pair[1] for pair in train_tagged_words])\n",
    "print(len(Tags_D),\"different tags used in the training data set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PRON', '.', 'CONJ', 'ADJ', 'NUM', 'VERB', 'ADP', 'NOUN', 'DET', 'PRT', 'X', 'ADV'}\n"
     ]
    }
   ],
   "source": [
    "# Printing different tags in the training set\n",
    "print(Tags_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging Algorithm - HMM\n",
    "\n",
    "We'll use the HMM algorithm to tag the words. Given a sequence of words to be tagged, the task is to assign the most probable tag to the word. \n",
    "\n",
    "In other words, to every word w, assign the tag t that maximises the likelihood P(t/w). Since P(t/w) = P(w/t). P(t) / P(w), after ignoring P(w), we have to compute P(w/t) and P(t).\n",
    "\n",
    "P(w/t) is basically the probability that given a tag (say NN), what is the probability of it being w (say 'building'). This can be computed by computing the fraction of all NNs which are equal to w, i.e. \n",
    "\n",
    "P(w/t) = count(w, t) / count(t). \n",
    "\n",
    "The term P(t) is the probability of tag t, and in a tagging task, we assume that a tag will depend only on the previous tag. In other words, the probability of a tag being NN will depend only on the previous tag t(n-1). So for e.g. if t(n-1) is a JJ, then t(n) is likely to be an NN since adjectives often precede a noun (blue coat, tall building etc.).\n",
    "\n",
    "Given the penn treebank tagged dataset, we can compute the two terms P(w/t) and P(t) and store them in two large matrices. The matrix of P(w/t) will be sparse, since each word will not be seen with most tags ever, and those terms will thus be zero. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unmodified Viterbi POS tagger\n",
    "### Build the vanilla Viterbi based POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Two Prior Probability\n",
    "   * Emission Probability - What is the probability of emission of a particular term\n",
    "   * Transition Probability - Given a particular tag what is the probability of the occurance of the next tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a matrix comprising of set of all Tags and Vocabulary\n",
    "#Row 'tags' represent Tags and Column 'vocab' represents Vocabulary\n",
    "# computing P(w/t) and storing in Tags_D x Vocab matrix\n",
    "tags = len(Tags_D)\n",
    "vocab = len(Vocab)\n",
    "w_given_t = np.zeros((tags, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(w_given_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emission Probability\n",
    "* Compute Word Given Tag -> Defining function here - For each word and tag that is given it returns a set of number of terms the word has been assign to the given tag and number of times this tag used in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_given_tag(word,tag,train_bag= train_tagged_words):\n",
    "    taglist = [pair for pair in train_bag if pair[1] == tag]\n",
    "    tag_count = len(taglist)    # Count the number of times tag has been used\n",
    "    w_in_tag = [pair[0] for pair in taglist if pair[0]==word]    #Counting set of all tag list pairs\n",
    "    word_count_given_tag = len(w_in_tag)    \n",
    "    \n",
    "    return (word_count_given_tag,tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 2612)\n",
      "(0, 27352)\n",
      "(0, 12910)\n"
     ]
    }
   ],
   "source": [
    "#Sample:\n",
    "print(word_given_tag('we','PRON'))\n",
    "print(word_given_tag('we','NOUN'))\n",
    "print(word_given_tag('we','VERB'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition Probability\n",
    "   * compute tag given tag: tag2(t2) given tag1 (t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1]) # Count Number of times t1 has appeared\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2: # Number of times next index for t1 - next tag after t1 is t2\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)\n",
    "\n",
    "# count_t2_t1 - count of the number of times t2 follows t1\n",
    "# count_t1 - Count of the number of times t1 is used in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2472, 11123)\n",
      "(1421, 12910)\n",
      "(1735, 12910)\n"
     ]
    }
   ],
   "source": [
    "# Samples:\n",
    "print(t2_given_t1(t2='NOUN', t1='.'))\n",
    "print(t2_given_t1(t2='NOUN', t1='VERB'))\n",
    "print(t2_given_t1(t2='DET', t1='VERB'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Transition Matrix - Square Matrix\n",
    "* Creating t x t transition matrix of tags\n",
    "* Each column is t2, each row is t1\n",
    "* Thus M(i, j) represents P(tj given ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_matrix = np.zeros((len(Tags_D), len(Tags_D)), dtype='float32')\n",
    "for i, t1 in enumerate(list(Tags_D)):\n",
    "    for j, t2 in enumerate(list(Tags_D)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.65696773e-03, 4.09647785e-02, 5.35987737e-03, 7.31240436e-02,\n",
       "        6.50842255e-03, 4.85451758e-01, 2.29709037e-02, 2.10949466e-01,\n",
       "        9.95405857e-03, 1.30168451e-02, 8.99693742e-02, 3.40735056e-02],\n",
       "       [6.63490072e-02, 9.33201462e-02, 5.75384349e-02, 4.39629592e-02,\n",
       "        8.10033232e-02, 8.90946686e-02, 9.13422629e-02, 2.22242206e-01,\n",
       "        1.73334539e-01, 2.42740265e-03, 2.69711409e-02, 5.23240119e-02],\n",
       "       [5.81125058e-02, 3.48675027e-02, 4.64900048e-04, 1.18084610e-01,\n",
       "        3.99814025e-02, 1.56671315e-01, 5.25337048e-02, 3.49139929e-01,\n",
       "        1.21338911e-01, 4.64900024e-03, 8.83310102e-03, 5.53231053e-02],\n",
       "       [3.29543574e-04, 6.39314577e-02, 1.69714950e-02, 6.64030313e-02,\n",
       "        2.12555602e-02, 1.16987973e-02, 7.82665983e-02, 6.99621022e-01,\n",
       "        4.94315382e-03, 1.07101668e-02, 2.10907888e-02, 4.77838190e-03],\n",
       "       [1.48898154e-03, 1.17331743e-01, 1.36986300e-02, 3.42465751e-02,\n",
       "        1.84931502e-01, 1.87611673e-02, 3.60333547e-02, 3.50208461e-01,\n",
       "        3.27575929e-03, 2.65038721e-02, 2.10541993e-01, 2.97796307e-03],\n",
       "       [3.57862115e-02, 3.49341594e-02, 5.57707204e-03, 6.49883822e-02,\n",
       "        2.28505041e-02, 1.69248641e-01, 9.20216888e-02, 1.10069714e-01,\n",
       "        1.34391949e-01, 3.06738969e-02, 2.17505813e-01, 8.19519758e-02],\n",
       "       [7.00310096e-02, 3.90249118e-02, 9.62258084e-04, 1.07024483e-01,\n",
       "        6.22260235e-02, 8.33956990e-03, 1.68929752e-02, 3.20966542e-01,\n",
       "        3.24708641e-01, 1.38992839e-03, 3.44274566e-02, 1.40062012e-02],\n",
       "       [4.60661016e-03, 2.40603983e-01, 4.26659845e-02, 1.22477328e-02,\n",
       "        9.54226404e-03, 1.47667453e-01, 1.76513597e-01, 2.63563901e-01,\n",
       "        1.29423812e-02, 4.33971919e-02, 2.91751977e-02, 1.70737058e-02],\n",
       "       [3.74350930e-03, 1.79929957e-02, 4.83033451e-04, 2.04323143e-01,\n",
       "        2.22195387e-02, 3.98502611e-02, 9.53991059e-03, 6.38087213e-01,\n",
       "        5.67564322e-03, 2.41516726e-04, 4.54051457e-02, 1.24381110e-02],\n",
       "       [1.77924223e-02, 4.38220762e-02, 2.30642501e-03, 8.30313042e-02,\n",
       "        5.66721596e-02, 4.05271828e-01, 2.00988464e-02, 2.47775942e-01,\n",
       "        9.78583172e-02, 1.64744642e-03, 1.35090612e-02, 1.02141676e-02],\n",
       "       [5.55378757e-02, 1.63590074e-01, 1.06619988e-02, 1.71865057e-02,\n",
       "        2.86441762e-03, 2.03851044e-01, 1.42584339e-01, 6.23806491e-02,\n",
       "        5.47422022e-02, 1.85232341e-01, 7.63844699e-02, 2.49840859e-02],\n",
       "       [1.49055980e-02, 1.37131497e-01, 6.95594586e-03, 1.29181847e-01,\n",
       "        3.04736663e-02, 3.43491226e-01, 1.18582316e-01, 3.14673744e-02,\n",
       "        6.98906928e-02, 1.42431268e-02, 2.31864862e-02, 8.04902315e-02]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better Readability converting the matrix to a data frame df \n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(Tags_D), index=list(Tags_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRON</th>\n",
       "      <th>.</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>NUM</th>\n",
       "      <th>VERB</th>\n",
       "      <th>ADP</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>DET</th>\n",
       "      <th>PRT</th>\n",
       "      <th>X</th>\n",
       "      <th>ADV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.007657</td>\n",
       "      <td>0.040965</td>\n",
       "      <td>0.005360</td>\n",
       "      <td>0.073124</td>\n",
       "      <td>0.006508</td>\n",
       "      <td>0.485452</td>\n",
       "      <td>0.022971</td>\n",
       "      <td>0.210949</td>\n",
       "      <td>0.009954</td>\n",
       "      <td>0.013017</td>\n",
       "      <td>0.089969</td>\n",
       "      <td>0.034074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.066349</td>\n",
       "      <td>0.093320</td>\n",
       "      <td>0.057538</td>\n",
       "      <td>0.043963</td>\n",
       "      <td>0.081003</td>\n",
       "      <td>0.089095</td>\n",
       "      <td>0.091342</td>\n",
       "      <td>0.222242</td>\n",
       "      <td>0.173335</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>0.026971</td>\n",
       "      <td>0.052324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.058113</td>\n",
       "      <td>0.034868</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.118085</td>\n",
       "      <td>0.039981</td>\n",
       "      <td>0.156671</td>\n",
       "      <td>0.052534</td>\n",
       "      <td>0.349140</td>\n",
       "      <td>0.121339</td>\n",
       "      <td>0.004649</td>\n",
       "      <td>0.008833</td>\n",
       "      <td>0.055323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.063931</td>\n",
       "      <td>0.016971</td>\n",
       "      <td>0.066403</td>\n",
       "      <td>0.021256</td>\n",
       "      <td>0.011699</td>\n",
       "      <td>0.078267</td>\n",
       "      <td>0.699621</td>\n",
       "      <td>0.004943</td>\n",
       "      <td>0.010710</td>\n",
       "      <td>0.021091</td>\n",
       "      <td>0.004778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.117332</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.034247</td>\n",
       "      <td>0.184932</td>\n",
       "      <td>0.018761</td>\n",
       "      <td>0.036033</td>\n",
       "      <td>0.350208</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>0.026504</td>\n",
       "      <td>0.210542</td>\n",
       "      <td>0.002978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.035786</td>\n",
       "      <td>0.034934</td>\n",
       "      <td>0.005577</td>\n",
       "      <td>0.064988</td>\n",
       "      <td>0.022851</td>\n",
       "      <td>0.169249</td>\n",
       "      <td>0.092022</td>\n",
       "      <td>0.110070</td>\n",
       "      <td>0.134392</td>\n",
       "      <td>0.030674</td>\n",
       "      <td>0.217506</td>\n",
       "      <td>0.081952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.070031</td>\n",
       "      <td>0.039025</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.107024</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.008340</td>\n",
       "      <td>0.016893</td>\n",
       "      <td>0.320967</td>\n",
       "      <td>0.324709</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.034427</td>\n",
       "      <td>0.014006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.004607</td>\n",
       "      <td>0.240604</td>\n",
       "      <td>0.042666</td>\n",
       "      <td>0.012248</td>\n",
       "      <td>0.009542</td>\n",
       "      <td>0.147667</td>\n",
       "      <td>0.176514</td>\n",
       "      <td>0.263564</td>\n",
       "      <td>0.012942</td>\n",
       "      <td>0.043397</td>\n",
       "      <td>0.029175</td>\n",
       "      <td>0.017074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.003744</td>\n",
       "      <td>0.017993</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.204323</td>\n",
       "      <td>0.022220</td>\n",
       "      <td>0.039850</td>\n",
       "      <td>0.009540</td>\n",
       "      <td>0.638087</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.045405</td>\n",
       "      <td>0.012438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.017792</td>\n",
       "      <td>0.043822</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.083031</td>\n",
       "      <td>0.056672</td>\n",
       "      <td>0.405272</td>\n",
       "      <td>0.020099</td>\n",
       "      <td>0.247776</td>\n",
       "      <td>0.097858</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.013509</td>\n",
       "      <td>0.010214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.055538</td>\n",
       "      <td>0.163590</td>\n",
       "      <td>0.010662</td>\n",
       "      <td>0.017187</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>0.203851</td>\n",
       "      <td>0.142584</td>\n",
       "      <td>0.062381</td>\n",
       "      <td>0.054742</td>\n",
       "      <td>0.185232</td>\n",
       "      <td>0.076384</td>\n",
       "      <td>0.024984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.014906</td>\n",
       "      <td>0.137131</td>\n",
       "      <td>0.006956</td>\n",
       "      <td>0.129182</td>\n",
       "      <td>0.030474</td>\n",
       "      <td>0.343491</td>\n",
       "      <td>0.118582</td>\n",
       "      <td>0.031467</td>\n",
       "      <td>0.069891</td>\n",
       "      <td>0.014243</td>\n",
       "      <td>0.023186</td>\n",
       "      <td>0.080490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          PRON         .      CONJ       ADJ       NUM      VERB       ADP  \\\n",
       "PRON  0.007657  0.040965  0.005360  0.073124  0.006508  0.485452  0.022971   \n",
       ".     0.066349  0.093320  0.057538  0.043963  0.081003  0.089095  0.091342   \n",
       "CONJ  0.058113  0.034868  0.000465  0.118085  0.039981  0.156671  0.052534   \n",
       "ADJ   0.000330  0.063931  0.016971  0.066403  0.021256  0.011699  0.078267   \n",
       "NUM   0.001489  0.117332  0.013699  0.034247  0.184932  0.018761  0.036033   \n",
       "VERB  0.035786  0.034934  0.005577  0.064988  0.022851  0.169249  0.092022   \n",
       "ADP   0.070031  0.039025  0.000962  0.107024  0.062226  0.008340  0.016893   \n",
       "NOUN  0.004607  0.240604  0.042666  0.012248  0.009542  0.147667  0.176514   \n",
       "DET   0.003744  0.017993  0.000483  0.204323  0.022220  0.039850  0.009540   \n",
       "PRT   0.017792  0.043822  0.002306  0.083031  0.056672  0.405272  0.020099   \n",
       "X     0.055538  0.163590  0.010662  0.017187  0.002864  0.203851  0.142584   \n",
       "ADV   0.014906  0.137131  0.006956  0.129182  0.030474  0.343491  0.118582   \n",
       "\n",
       "          NOUN       DET       PRT         X       ADV  \n",
       "PRON  0.210949  0.009954  0.013017  0.089969  0.034074  \n",
       ".     0.222242  0.173335  0.002427  0.026971  0.052324  \n",
       "CONJ  0.349140  0.121339  0.004649  0.008833  0.055323  \n",
       "ADJ   0.699621  0.004943  0.010710  0.021091  0.004778  \n",
       "NUM   0.350208  0.003276  0.026504  0.210542  0.002978  \n",
       "VERB  0.110070  0.134392  0.030674  0.217506  0.081952  \n",
       "ADP   0.320967  0.324709  0.001390  0.034427  0.014006  \n",
       "NOUN  0.263564  0.012942  0.043397  0.029175  0.017074  \n",
       "DET   0.638087  0.005676  0.000242  0.045405  0.012438  \n",
       "PRT   0.247776  0.097858  0.001647  0.013509  0.010214  \n",
       "X     0.062381  0.054742  0.185232  0.076384  0.024984  \n",
       "ADV   0.031467  0.069891  0.014243  0.023186  0.080490  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize transition probability as matrix as a heatmap to check how each of the probability looks like\n",
    "* Heatmap of tags matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 12))\n",
    "sns.heatmap(tags_df,cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crisp:\n",
    "    * NOUN following ADJ has one of the highest probabilities in the entire matrix\n",
    "    * Second highset NOUN followed by DET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Viterbi Algorithm\n",
    "* It takes greddy heuristics - It starts from start of the sentence and at every step maximizes the likelihood of this tag sequence that is likely to be assigned to the sentence so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set of all Tagged Words 95547\n"
     ]
    }
   ],
   "source": [
    "# Printing Set of ll tagged words\n",
    "print(\"Set of all Tagged Words\",len(train_tagged_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining function called viterbi\n",
    "\n",
    "* Iterating through the given sequence of words\n",
    "* Calculating the product of transition and emission probabilties for all possible tags(each word starting from the sequence)\n",
    "* Assiging the tag which has maximum probability obtained above and then move to next word to repeat the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic - Vanilla Viterbi Algorithm\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    # T - Set of all tags that we are going to create\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    # For each set of words - Taking the index represents variable 'key'\n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0: # If the index is Zero\n",
    "                transition_p = tags_df.loc['.', tag] # Taking dot '.' as the previous tag\n",
    "            else:\n",
    "                # Otherwise taking state of -1. state is the list of all states seen so far\n",
    "                # '-1' represents previous state\n",
    "                transition_p = tags_df.loc[state[-1], tag] \n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p  \n",
    "            #Appending state probability to the list 'p' which contains the set of all states for which we are computing the state probability\n",
    "            #Set of all states essentially the set of all tags\n",
    "            p.append(state_probability)\n",
    "        \n",
    "        #Computing maximum value of the p\n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)]# Index of the maximum value is the tag 'T' and assigning to variable 'state_max'\n",
    "        state.append(state_max)     # appending to state\n",
    "    # Returning list of pairs comprising of words there in the sentence and state assign to this word\n",
    "    return list(zip(words, state))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Unmodified Vanilla Viterbi POS taggeon Validation Set\n",
    "    * Running on entire test dataset would take more than 4 hours\n",
    "    * Evaluating Vanilla viterbi algorithm on a few sample sentences of the validation(test) data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234) #  starting point in generating random numbers\n",
    "# choose random 5 sentences\n",
    "random_num = [random.randint(1,len(validation_set)) for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of sentences\n",
    "test_run = [validation_set[i] for i in random_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking list of all tagged words\n",
    "test_run_base = [tup for sent in test_run for tup in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'DET'),\n",
       "  ('Contra', 'NOUN'),\n",
       "  ('military', 'ADJ'),\n",
       "  ('command', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('in', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('statement', 'NOUN'),\n",
       "  ('from', 'ADP'),\n",
       "  ('Honduras', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('said', 'VERB'),\n",
       "  ('0', 'X'),\n",
       "  ('Sandinista', 'NOUN'),\n",
       "  ('troops', 'NOUN'),\n",
       "  ('had', 'VERB'),\n",
       "  ('launched', 'VERB'),\n",
       "  ('a', 'DET'),\n",
       "  ('major', 'ADJ'),\n",
       "  ('offensive', 'NOUN'),\n",
       "  ('against', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('rebel', 'NOUN'),\n",
       "  ('forces', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('*-1', 'X'),\n",
       "  ('Bucking', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('market', 'NOUN'),\n",
       "  ('trend', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('an', 'DET'),\n",
       "  ('issue', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('$', '.'),\n",
       "  ('130', 'NUM'),\n",
       "  ('million', 'NUM'),\n",
       "  ('*U*', 'X'),\n",
       "  ('general', 'ADJ'),\n",
       "  ('obligation', 'NOUN'),\n",
       "  ('distributable', 'ADJ'),\n",
       "  ('state', 'NOUN'),\n",
       "  ('aid', 'NOUN'),\n",
       "  ('bonds', 'NOUN'),\n",
       "  ('from', 'ADP'),\n",
       "  ('Detroit', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('Mich.', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('apparently', 'ADV'),\n",
       "  ('drew', 'VERB'),\n",
       "  ('solid', 'ADJ'),\n",
       "  ('investor', 'NOUN'),\n",
       "  ('interest', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('Ralston', 'NOUN'),\n",
       "  ('said', 'VERB'),\n",
       "  ('0', 'X'),\n",
       "  ('its', 'PRON'),\n",
       "  ('Eveready', 'NOUN'),\n",
       "  ('battery', 'NOUN'),\n",
       "  ('unit', 'NOUN'),\n",
       "  ('was', 'VERB'),\n",
       "  ('hurt', 'VERB'),\n",
       "  ('*-1', 'X'),\n",
       "  ('by', 'ADP'),\n",
       "  ('continuing', 'VERB'),\n",
       "  ('economic', 'ADJ'),\n",
       "  ('problems', 'NOUN'),\n",
       "  ('in', 'ADP'),\n",
       "  ('South', 'NOUN'),\n",
       "  ('America', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('I', 'PRON'),\n",
       "  ('feel', 'VERB'),\n",
       "  ('pretty', 'ADV'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('about', 'ADP'),\n",
       "  ('it', 'PRON'),\n",
       "  ('.', '.')],\n",
       " [('Mr.', 'NOUN'),\n",
       "  ('Felten', 'NOUN'),\n",
       "  ('said', 'VERB'),\n",
       "  (',', '.'),\n",
       "  ('``', '.'),\n",
       "  ('We', 'PRON'),\n",
       "  ('got', 'VERB'),\n",
       "  ('what', 'PRON'),\n",
       "  ('*T*-252', 'X'),\n",
       "  ('amounted', 'VERB'),\n",
       "  ('to', 'PRT'),\n",
       "  ('a', 'DET'),\n",
       "  ('parking', 'NOUN'),\n",
       "  ('ticket', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('by', 'ADP'),\n",
       "  ('*-1', 'X'),\n",
       "  ('complaining', 'VERB'),\n",
       "  ('about', 'ADP'),\n",
       "  ('it', 'PRON'),\n",
       "  (',', '.'),\n",
       "  ('we', 'PRON'),\n",
       "  ('ended', 'VERB'),\n",
       "  ('up', 'PRT'),\n",
       "  ('with', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('sizable', 'ADJ'),\n",
       "  ('fine', 'NOUN'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('suspension', 'NOUN'),\n",
       "  ('.', '.'),\n",
       "  (\"''\", '.')]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking list of all untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n",
    "test_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  18.35\n"
     ]
    }
   ],
   "source": [
    "# Tagging the test sentences - Calling Viterbi function\n",
    "# Using start time and end time to measure the timetaken\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "print(\"Time taken in seconds: \", round(difference,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('Contra', 'PRON'), ('military', 'ADJ'), ('command', 'VERB'), (',', '.'), ('in', 'ADP'), ('a', 'DET'), ('statement', 'NOUN'), ('from', 'ADP'), ('Honduras', 'PRON'), (',', '.'), ('said', 'VERB'), ('0', 'X'), ('Sandinista', 'PRON'), ('troops', 'NOUN'), ('had', 'VERB'), ('launched', 'VERB'), ('a', 'DET'), ('major', 'ADJ'), ('offensive', 'PRON'), ('against', 'ADP'), ('the', 'DET'), ('rebel', 'PRON'), ('forces', 'VERB'), ('.', '.'), ('*-1', 'X'), ('Bucking', 'PRON'), ('the', 'DET'), ('market', 'NOUN'), ('trend', 'NOUN'), (',', '.'), ('an', 'DET'), ('issue', 'NOUN'), ('of', 'ADP'), ('$', '.'), ('130', 'NUM'), ('million', 'NUM'), ('*U*', 'X'), ('general', 'ADJ'), ('obligation', 'NOUN'), ('distributable', 'ADJ'), ('state', 'NOUN'), ('aid', 'NOUN'), ('bonds', 'NOUN'), ('from', 'ADP'), ('Detroit', 'NOUN'), (',', '.'), ('Mich.', 'NOUN'), (',', '.'), ('apparently', 'ADV'), ('drew', 'PRON'), ('solid', 'ADJ'), ('investor', 'NOUN'), ('interest', 'NOUN'), ('.', '.'), ('Ralston', 'NOUN'), ('said', 'VERB'), ('0', 'X'), ('its', 'PRON'), ('Eveready', 'PRON'), ('battery', 'NOUN'), ('unit', 'NOUN'), ('was', 'VERB'), ('hurt', 'VERB'), ('*-1', 'X'), ('by', 'ADP'), ('continuing', 'VERB'), ('economic', 'ADJ'), ('problems', 'NOUN'), ('in', 'ADP'), ('South', 'NOUN'), ('America', 'NOUN'), ('.', '.'), ('I', 'PRON'), ('feel', 'VERB'), ('pretty', 'ADV'), ('good', 'ADJ'), ('about', 'ADP'), ('it', 'PRON'), ('.', '.'), ('Mr.', 'NOUN'), ('Felten', 'NOUN'), ('said', 'VERB'), (',', '.'), ('``', '.'), ('We', 'PRON'), ('got', 'VERB'), ('what', 'PRON'), ('*T*-252', 'PRON'), ('amounted', 'VERB'), ('to', 'PRT'), ('a', 'DET'), ('parking', 'NOUN'), ('ticket', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('by', 'ADP'), ('*-1', 'X'), ('complaining', 'PRON'), ('about', 'ADP'), ('it', 'PRON'), (',', '.'), ('we', 'PRON'), ('ended', 'VERB'), ('up', 'ADV'), ('with', 'ADP'), ('a', 'DET'), ('sizable', 'ADJ'), ('fine', 'NOUN'), ('and', 'CONJ'), ('suspension', 'NOUN'), ('.', '.'), (\"''\", '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this check array which takes set of all pairs from the tagged sequence and test_run_base\n",
    "# Only if the first and the second element have the same tag  then we note it down in the list\n",
    "\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy - Number of correct answers divided by Total number of tags\n",
    "accuracy = len(check)/len(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Vanilla Veterbi Algorithm : 88.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Vanilla Veterbi Algorithm :\", round(accuracy*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Incorrect Tagged cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking set of all pairs where the tag that is assigned by us is not same as the tag assigned in the dataset\n",
    "incorrect_tagged_words = [[test_run_base[i-1],j] for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'DET'), (('Contra', 'PRON'), ('Contra', 'NOUN'))], [('military', 'ADJ'), (('command', 'VERB'), ('command', 'NOUN'))], [('from', 'ADP'), (('Honduras', 'PRON'), ('Honduras', 'NOUN'))], [('0', 'X'), (('Sandinista', 'PRON'), ('Sandinista', 'NOUN'))], [('major', 'ADJ'), (('offensive', 'PRON'), ('offensive', 'NOUN'))], [('the', 'DET'), (('rebel', 'PRON'), ('rebel', 'NOUN'))], [('rebel', 'NOUN'), (('forces', 'VERB'), ('forces', 'NOUN'))], [('*-1', 'X'), (('Bucking', 'PRON'), ('Bucking', 'VERB'))], [('apparently', 'ADV'), (('drew', 'PRON'), ('drew', 'VERB'))], [('its', 'PRON'), (('Eveready', 'PRON'), ('Eveready', 'NOUN'))], [('what', 'PRON'), (('*T*-252', 'PRON'), ('*T*-252', 'X'))], [('*-1', 'X'), (('complaining', 'PRON'), ('complaining', 'VERB'))], [('ended', 'VERB'), (('up', 'ADV'), ('up', 'PRT'))]]\n"
     ]
    }
   ],
   "source": [
    "print(incorrect_tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "* The Time taken and accuracy of Unmodified Viterbi POS tagger - Vanilla Viterbi Algorithm\n",
    "        * Time Taken  - Approximately 20-25 Seconds\n",
    "        * Accuracy    - 88.5 %\n",
    "    \n",
    "* Few Incorrect Tagged Cases\n",
    "        * Contra   - 'NUM'\n",
    "        * Honduras - 'NUM'\n",
    "        * Bucking  - 'NUM'\n",
    "        * Complaining - 'NUM'\n",
    "        * Sandinista  - 'NUM'\n",
    "        * Offensive   - 'NUM'\n",
    "        etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the problem of unknown words\n",
    "* It assigned an incorrect tag arbitrarily\n",
    "* We can see that most of the unknown words have been tagged as 'NUM' \n",
    "* This is because, for unknown words, the emission probabilities for all candidate tags are 0, so the algorithm arbitrarily chooses (the first) tag in the list which is 'NUM'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Modification-Technique I\n",
    "    * Assigining based on Transition probability only if emission probability of unknown word is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Modification Algorithm - Technique 1\n",
    "def Viterbi_Technique1(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "\t\n",
    "\t# T - Set of all tags that we are going to create\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "\t# For each set of words - Taking the index represents variable 'key'\n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "\t\t\n",
    "\t\t# Creating list for storing transition probabilities\n",
    "\t\t\n",
    "        p_transition =[] \n",
    "\t\t\n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag] # Taking dot '.' as the previous tag\n",
    "            else:\n",
    "\t\t\t\t# Otherwise taking state of -1. state is the list of all states seen so far\t\n",
    "                # '-1' represents previous state\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p \n",
    "\t\t\t\n",
    "            #Appending state probability to the list 'p' which contains the set of all states for which we are computing the state probability\n",
    "            #Set of all states essentially the set of all tags\t\n",
    "            p.append(state_probability)\n",
    "\t\t\t\n",
    "\t\t\t#Appending Transition probability to the list p_transition\n",
    "            p_transition.append(transition_p)\n",
    "        \n",
    "\t\t#Computing maximum value of the p\t\t\n",
    "        pmax = max(p)\n",
    "\t\t\n",
    "\t\t# Index of the maximum value is the tag 'T' and assigning to variable 'state_max'\n",
    "        state_max = T[p.index(pmax)] \n",
    "        \n",
    "      \n",
    "        # if it is unknown word (probability is zero) then use the transition probability\n",
    "        if(pmax==0):\n",
    "            pmax = max(p_transition)\n",
    "            state_max = T[p_transition.index(pmax)]\n",
    "                           \n",
    "        else:  # appending to state\n",
    "            state_max = T[p.index(pmax)] \n",
    "        \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Viterbi Modification-Technique I on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  24.6\n"
     ]
    }
   ],
   "source": [
    "# Tagging the test sentences - Calling Viterbi Technique 1 function\n",
    "# Using start time and end time to measure the timetaken\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi_Technique1(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", round(difference,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('Contra', 'NOUN'), ('military', 'ADJ'), ('command', 'VERB'), (',', '.'), ('in', 'ADP'), ('a', 'DET'), ('statement', 'NOUN'), ('from', 'ADP'), ('Honduras', 'DET'), (',', '.'), ('said', 'VERB'), ('0', 'X'), ('Sandinista', 'VERB'), ('troops', 'NOUN'), ('had', 'VERB'), ('launched', 'VERB'), ('a', 'DET'), ('major', 'ADJ'), ('offensive', 'NOUN'), ('against', 'ADP'), ('the', 'DET'), ('rebel', 'NOUN'), ('forces', 'NOUN'), ('.', '.'), ('*-1', 'X'), ('Bucking', 'VERB'), ('the', 'DET'), ('market', 'NOUN'), ('trend', 'NOUN'), (',', '.'), ('an', 'DET'), ('issue', 'NOUN'), ('of', 'ADP'), ('$', '.'), ('130', 'NUM'), ('million', 'NUM'), ('*U*', 'X'), ('general', 'ADJ'), ('obligation', 'NOUN'), ('distributable', 'ADJ'), ('state', 'NOUN'), ('aid', 'NOUN'), ('bonds', 'NOUN'), ('from', 'ADP'), ('Detroit', 'NOUN'), (',', '.'), ('Mich.', 'NOUN'), (',', '.'), ('apparently', 'ADV'), ('drew', 'VERB'), ('solid', 'ADJ'), ('investor', 'NOUN'), ('interest', 'NOUN'), ('.', '.'), ('Ralston', 'NOUN'), ('said', 'VERB'), ('0', 'X'), ('its', 'PRON'), ('Eveready', 'VERB'), ('battery', 'NOUN'), ('unit', 'NOUN'), ('was', 'VERB'), ('hurt', 'VERB'), ('*-1', 'X'), ('by', 'ADP'), ('continuing', 'VERB'), ('economic', 'ADJ'), ('problems', 'NOUN'), ('in', 'ADP'), ('South', 'NOUN'), ('America', 'NOUN'), ('.', '.'), ('I', 'PRON'), ('feel', 'VERB'), ('pretty', 'ADV'), ('good', 'ADJ'), ('about', 'ADP'), ('it', 'PRON'), ('.', '.'), ('Mr.', 'NOUN'), ('Felten', 'NOUN'), ('said', 'VERB'), (',', '.'), ('``', '.'), ('We', 'PRON'), ('got', 'VERB'), ('what', 'PRON'), ('*T*-252', 'VERB'), ('amounted', 'VERB'), ('to', 'PRT'), ('a', 'DET'), ('parking', 'NOUN'), ('ticket', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('by', 'ADP'), ('*-1', 'X'), ('complaining', 'VERB'), ('about', 'ADP'), ('it', 'PRON'), (',', '.'), ('we', 'PRON'), ('ended', 'VERB'), ('up', 'ADV'), ('with', 'ADP'), ('a', 'DET'), ('sizable', 'ADJ'), ('fine', 'NOUN'), ('and', 'CONJ'), ('suspension', 'NOUN'), ('.', '.'), (\"''\", '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this check array which takes set of all pairs from the tagged sequence and test_run_base\n",
    "# Only if the first and the second element have the same tag  then we note it down in the list\n",
    "\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy - Number of correct answers divided by Total number of tags\n",
    "accuracy = len(check)/len(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Viterbi Modification-Technique I : 94.69\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Viterbi Modification-Technique I :\", round(accuracy*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking set of all pairs where the tag that is assigned by us is not same as the tag assigned in the dataset\n",
    "incorrect_tagged_words = [[test_run_base[i-1],j] for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('military', 'ADJ'), (('command', 'VERB'), ('command', 'NOUN'))], [('from', 'ADP'), (('Honduras', 'DET'), ('Honduras', 'NOUN'))], [('0', 'X'), (('Sandinista', 'VERB'), ('Sandinista', 'NOUN'))], [('its', 'PRON'), (('Eveready', 'VERB'), ('Eveready', 'NOUN'))], [('what', 'PRON'), (('*T*-252', 'VERB'), ('*T*-252', 'X'))], [('ended', 'VERB'), (('up', 'ADV'), ('up', 'PRT'))]]\n"
     ]
    }
   ],
   "source": [
    "print(incorrect_tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "* The Time taken and accuracy of Viterbi Modification-Technique I:\n",
    "        * Time Taken  - Approximately 24 Seconds\n",
    "        * Accuracy    - 94.69 %\n",
    "* We got better Accuracy in Viterbi Modification-Technique I compared to Unmodified Vanilla Viterbi POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Modification-Technique I - Further Modification in Technique 1\n",
    "\n",
    "* Adding Tag occurance probability weights: we will apply weights based on the probability of tag occurance to the transition probabilities of tags and then use the resulting probability for predicting unknown words.\n",
    "* This scheme will also take into account that some POS tags are more likely to occur as compared to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PRON', 0.0273373313657153),\n",
       " ('.', 0.11641391147812072),\n",
       " ('CONJ', 0.02251248076862696),\n",
       " ('ADJ', 0.06351847781719991),\n",
       " ('NUM', 0.035145007169246546),\n",
       " ('VERB', 0.1351167488251855),\n",
       " ('ADP', 0.0978889970381069),\n",
       " ('NOUN', 0.2862674913916711),\n",
       " ('DET', 0.08666938784053921),\n",
       " ('PRT', 0.03176447193527793),\n",
       " ('X', 0.06576867928872701),\n",
       " ('ADV', 0.031597015081582885)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating list containing tuples of POS tags and POS tag occurance probability\n",
    "prob_tup = []\n",
    "AllTags = len([tag for word,tag in train_tagged_words])\n",
    "for t in Tags_D:\n",
    "    each_tag = [tag for word,tag in train_tagged_words if tag==t]\n",
    "    prob_tup.append((t,len(each_tag)/AllTags))\n",
    "\n",
    "prob_tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Modification-Technique I - Further Modification in Technique 1\n",
    "def Viterbi_Technique1_Modified(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "\t\n",
    "\t# T - Set of all tags that we are going to create\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "\t# For each set of words - Taking the index represents variable 'key'\n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "\t\t\n",
    "\t\t# Creating list for storing transition probabilities\n",
    "\t\t\n",
    "        p_transition =[] # Store Transition Probability\n",
    "\t\t\n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag] # Taking dot '.' as the previous tag\n",
    "            else:\n",
    "\t\t\t\t# Otherwise taking state of -1. state is the list of all states seen so far\t\n",
    "                # '-1' represents previous state\n",
    "\t\t\t\t\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p \n",
    "\t\t\t\n",
    "            #Appending state probability to the list 'p' which contains the set of all states for which we are computing the state probability\n",
    "            #Set of all states essentially the set of all tags\t\n",
    "            p.append(state_probability)\n",
    "\t\t\t\n",
    "            # Finding Occurance probability of POS Tag\n",
    "\n",
    "            Occurance_tag_p = [pair[1] for pair in prob_tup if pair[0]==tag ]\n",
    "\n",
    "            # calculate the transition prob weighted by tag occurance probability.\n",
    "\t\n",
    "            transition_p = Occurance_tag_p[0]*transition_p \n",
    "\t\t\t\n",
    "\t\t\t#Appending Transition probability to the list p_transition\n",
    "            p_transition.append(transition_p)\n",
    "        \n",
    "\t\t#Computing maximum value of the p\t\t\n",
    "        pmax = max(p)\n",
    "\t\t\n",
    "\t\t# Index of the maximum value is the tag 'T' and assigning to variable 'state_max'\n",
    "        state_max = T[p.index(pmax)] \n",
    "        \n",
    "      \n",
    "        # if it is unknown word (probability is zero) then use the weighted transition probability\n",
    "        if(pmax==0):\n",
    "            pmax = max(p_transition)\n",
    "            state_max = T[p_transition.index(pmax)]\n",
    "                           \n",
    "        else:  # appending to state\n",
    "            state_max = T[p.index(pmax)] \n",
    "        \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Viterbi Modification-Technique I - Further Modification in Technique 1 on Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  18.4\n"
     ]
    }
   ],
   "source": [
    "# Tagging the test sentences - Calling Viterbi Technique 1 function\n",
    "# Using start time and end time to measure the timetaken\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi_Technique1_Modified(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", round(difference,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('Contra', 'NOUN'), ('military', 'ADJ'), ('command', 'VERB'), (',', '.'), ('in', 'ADP'), ('a', 'DET'), ('statement', 'NOUN'), ('from', 'ADP'), ('Honduras', 'NOUN'), (',', '.'), ('said', 'VERB'), ('0', 'X'), ('Sandinista', 'VERB'), ('troops', 'NOUN'), ('had', 'VERB'), ('launched', 'VERB'), ('a', 'DET'), ('major', 'ADJ'), ('offensive', 'NOUN'), ('against', 'ADP'), ('the', 'DET'), ('rebel', 'NOUN'), ('forces', 'NOUN'), ('.', '.'), ('*-1', 'X'), ('Bucking', 'VERB'), ('the', 'DET'), ('market', 'NOUN'), ('trend', 'NOUN'), (',', '.'), ('an', 'DET'), ('issue', 'NOUN'), ('of', 'ADP'), ('$', '.'), ('130', 'NUM'), ('million', 'NUM'), ('*U*', 'X'), ('general', 'ADJ'), ('obligation', 'NOUN'), ('distributable', 'ADJ'), ('state', 'NOUN'), ('aid', 'NOUN'), ('bonds', 'NOUN'), ('from', 'ADP'), ('Detroit', 'NOUN'), (',', '.'), ('Mich.', 'NOUN'), (',', '.'), ('apparently', 'ADV'), ('drew', 'VERB'), ('solid', 'ADJ'), ('investor', 'NOUN'), ('interest', 'NOUN'), ('.', '.'), ('Ralston', 'NOUN'), ('said', 'VERB'), ('0', 'X'), ('its', 'PRON'), ('Eveready', 'VERB'), ('battery', 'NOUN'), ('unit', 'NOUN'), ('was', 'VERB'), ('hurt', 'VERB'), ('*-1', 'X'), ('by', 'ADP'), ('continuing', 'VERB'), ('economic', 'ADJ'), ('problems', 'NOUN'), ('in', 'ADP'), ('South', 'NOUN'), ('America', 'NOUN'), ('.', '.'), ('I', 'PRON'), ('feel', 'VERB'), ('pretty', 'ADV'), ('good', 'ADJ'), ('about', 'ADP'), ('it', 'PRON'), ('.', '.'), ('Mr.', 'NOUN'), ('Felten', 'NOUN'), ('said', 'VERB'), (',', '.'), ('``', '.'), ('We', 'PRON'), ('got', 'VERB'), ('what', 'PRON'), ('*T*-252', 'VERB'), ('amounted', 'VERB'), ('to', 'PRT'), ('a', 'DET'), ('parking', 'NOUN'), ('ticket', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('by', 'ADP'), ('*-1', 'X'), ('complaining', 'VERB'), ('about', 'ADP'), ('it', 'PRON'), (',', '.'), ('we', 'PRON'), ('ended', 'VERB'), ('up', 'ADV'), ('with', 'ADP'), ('a', 'DET'), ('sizable', 'ADJ'), ('fine', 'NOUN'), ('and', 'CONJ'), ('suspension', 'NOUN'), ('.', '.'), (\"''\", '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this check array which takes set of all pairs from the tagged sequence and test_run_base\n",
    "# Only if the first and the second element have the same tag  then we note it down in the list\n",
    "\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy - Number of correct answers divided by Total number of tags\n",
    "accuracy = len(check)/len(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Viterbi Modification-Technique I - Further Modification : 95.57522123893806\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Viterbi Modification-Technique I - Further Modification :\", accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking set of all pairs where the tag that is assigned by us is not same as the tag assigned in the dataset\n",
    "incorrect_tagged_words = [[test_run_base[i-1],j] for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('military', 'ADJ'), (('command', 'VERB'), ('command', 'NOUN'))], [('0', 'X'), (('Sandinista', 'VERB'), ('Sandinista', 'NOUN'))], [('its', 'PRON'), (('Eveready', 'VERB'), ('Eveready', 'NOUN'))], [('what', 'PRON'), (('*T*-252', 'VERB'), ('*T*-252', 'X'))], [('ended', 'VERB'), (('up', 'ADV'), ('up', 'PRT'))]]\n"
     ]
    }
   ],
   "source": [
    "print(incorrect_tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "* The Time taken and accuracy of Viterbi Modification-Technique I - Further Modification:\n",
    "        * Time Taken  - Approximately 19 Seconds\n",
    "        * Accuracy    - 95.57 %\n",
    "* We got better Accuracy comapred to previous one\n",
    "* Below words are correctly POS Tagged by Viterbi Modification-Technique I\n",
    "        * Contra     - Vanilla Viterbi|'NUM' - Viterbi Technique I|'NOUN'\n",
    "        * Honduras   - Vanilla Viterbi|'NUM' - Viterbi Technique I|'NOUN'\n",
    "        * Bucking    - Vanilla Viterbi|'NUM' - Viterbi Technique I|'VERB'\n",
    "        * complaining- Vanilla Viterbi|'NUM' - Viterbi Technique I|'VERB'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Modification - Technique II - Rule Based Tagger\n",
    "\n",
    "* We also observed numbers are tagging as 'X'\n",
    "* Word ends with ing tagging as adjective (eg:'outstanding', 'ADJ')- Few verbs tagging as NOUN \n",
    "* Word *T*-252' tagging as 'VERB' instead 'X'\n",
    "* Captilized word can still be defaulted as 'NOUN' using regex in case if it is not satisfying any rule\n",
    "* Past tense words can be handled by regex\n",
    "* These kind special problems can be rectified in regular expression(regex) rule. We can extract it only based on the rule\n",
    "* As we know rule based tagger by defaults assigns as 'NOUN' if rules does not fall in any rule. This can also be handled in regular expression -  To correct, assign the tags for any such word based purely on transition probability of tags by modifying the rule based tagger output to 'NN' instead of 'NOUN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Regex rules and rule based tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Regular Expression rules (regex) for tagging\n",
    "regex_rules = [\n",
    "        (r'.*ing$', 'VERB'),              # gerund\n",
    "    (r'.*ed$', 'VERB'),               # past tense \n",
    "    (r'.*es$', 'VERB'),               # verb    \n",
    "    (r'.*\\'s$', 'NOUN'),              # possessive nouns\n",
    "    (r'.*s$', 'NOUN'),                # plural nouns\n",
    "    (r'\\*T?\\*?-[0-9]+$', 'X'),        # X\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'), # cardinal numbers\n",
    "    (r'^[A-Z][a-z].*', 'NOUN'),       # NOUN\n",
    "    (r'.*', 'NN')                     # default\n",
    "]\n",
    "\n",
    "RegEx_Rule_Based_Tagger = nltk.RegexpTagger(regex_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Modification - Technique II \n",
    "def Viterbi_Technique2(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "\t\n",
    "\t# T - Set of all tags that we are going to create\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "\t# For each set of words - Taking the index represents variable 'key'\n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "\t\t\n",
    "\t\t# Creating list for storing transition probabilities\n",
    "\t\t\n",
    "        p_transition =[] \n",
    "\t\t\n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag] # Taking dot '.' as the previous tag\n",
    "            else:\n",
    "\t\t\t\t# Otherwise taking state of -1. state is the list of all states seen so far\t\n",
    "                # '-1' represents previous state\n",
    "\t\t\t\t\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p \n",
    "\t\t\t\n",
    "            #Appending state probability to the list 'p' which contains the set of all states for which we are computing the state probability\n",
    "            #Set of all states essentially the set of all tags\t\n",
    "            p.append(state_probability)\n",
    "\t\t\t\n",
    "            # Finding Occurance probability of POS Tag\n",
    "\n",
    "            Occurance_tag_p = [pair[1] for pair in prob_tup if pair[0]==tag ]\n",
    "\n",
    "            # calculate the transition prob weighted by tag occurance probability.\n",
    "\t\n",
    "            transition_p = Occurance_tag_p[0]*transition_p \n",
    "\t\t\t\n",
    "\t\t\t#Appending Transition probability to the list p_transition\n",
    "            p_transition.append(transition_p)\n",
    "        \n",
    "\t\t#Computing maximum value of the p\t\t\n",
    "        pmax = max(p)\n",
    "\t\t\n",
    "\t\t# Index of the maximum value is the tag 'T' and assigning to variable 'state_max'\n",
    "        state_max = RegEx_Rule_Based_Tagger.tag([word])[0][1]\n",
    "        \n",
    "        # Taking state - Probability Maximum\n",
    "        if(pmax==0):\n",
    "            state_max = RegEx_Rule_Based_Tagger.tag([word])[0][1] # assign based on rule based tagger\n",
    "        \n",
    "            # if unknown word does not satisfy any rule, find the tag with maximum transition probability\n",
    "            if state_max == 'NN':\n",
    "                pmax = max(p_transition)\n",
    "                state_max = T[p_transition.index(pmax)]                 \n",
    "                \n",
    "        else:\n",
    "             if state_max != 'X':\n",
    "                # getting state for which probability is maximum\n",
    "                state_max = T[p.index(pmax)] \n",
    "        \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  18.5\n"
     ]
    }
   ],
   "source": [
    "# Tagging the test sentences - Calling Viterbi Technique 1 function\n",
    "# Using start time and end time to measure the timetaken\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi_Technique2(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", round(difference,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('Contra', 'NOUN'), ('military', 'ADJ'), ('command', 'VERB'), (',', '.'), ('in', 'ADP'), ('a', 'DET'), ('statement', 'NOUN'), ('from', 'ADP'), ('Honduras', 'NOUN'), (',', '.'), ('said', 'VERB'), ('0', 'X'), ('Sandinista', 'NOUN'), ('troops', 'NOUN'), ('had', 'VERB'), ('launched', 'VERB'), ('a', 'DET'), ('major', 'ADJ'), ('offensive', 'NOUN'), ('against', 'ADP'), ('the', 'DET'), ('rebel', 'NOUN'), ('forces', 'NOUN'), ('.', '.'), ('*-1', 'X'), ('Bucking', 'VERB'), ('the', 'DET'), ('market', 'NOUN'), ('trend', 'NOUN'), (',', '.'), ('an', 'DET'), ('issue', 'NOUN'), ('of', 'ADP'), ('$', '.'), ('130', 'NUM'), ('million', 'NUM'), ('*U*', 'X'), ('general', 'ADJ'), ('obligation', 'NOUN'), ('distributable', 'ADJ'), ('state', 'NOUN'), ('aid', 'NOUN'), ('bonds', 'NOUN'), ('from', 'ADP'), ('Detroit', 'NOUN'), (',', '.'), ('Mich.', 'NOUN'), (',', '.'), ('apparently', 'ADV'), ('drew', 'VERB'), ('solid', 'ADJ'), ('investor', 'NOUN'), ('interest', 'NOUN'), ('.', '.'), ('Ralston', 'NOUN'), ('said', 'VERB'), ('0', 'X'), ('its', 'PRON'), ('Eveready', 'NOUN'), ('battery', 'NOUN'), ('unit', 'NOUN'), ('was', 'VERB'), ('hurt', 'VERB'), ('*-1', 'X'), ('by', 'ADP'), ('continuing', 'VERB'), ('economic', 'ADJ'), ('problems', 'NOUN'), ('in', 'ADP'), ('South', 'NOUN'), ('America', 'NOUN'), ('.', '.'), ('I', 'PRON'), ('feel', 'VERB'), ('pretty', 'ADV'), ('good', 'ADJ'), ('about', 'ADP'), ('it', 'PRON'), ('.', '.'), ('Mr.', 'NOUN'), ('Felten', 'NOUN'), ('said', 'VERB'), (',', '.'), ('``', '.'), ('We', 'PRON'), ('got', 'VERB'), ('what', 'PRON'), ('*T*-252', 'X'), ('amounted', 'VERB'), ('to', 'PRT'), ('a', 'DET'), ('parking', 'NOUN'), ('ticket', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('by', 'ADP'), ('*-1', 'X'), ('complaining', 'VERB'), ('about', 'ADP'), ('it', 'PRON'), (',', '.'), ('we', 'PRON'), ('ended', 'VERB'), ('up', 'ADV'), ('with', 'ADP'), ('a', 'DET'), ('sizable', 'ADJ'), ('fine', 'NOUN'), ('and', 'CONJ'), ('suspension', 'NOUN'), ('.', '.'), (\"''\", '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this check array which takes set of all pairs from the tagged sequence and test_run_base\n",
    "# Only if the first and the second element have the same tag  then we note it down in the list\n",
    "\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy - Number of correct answers divided by Total number of tags\n",
    "accuracy = len(check)/len(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Viterbi Modification - Technique II Algorithm : 98.23\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Viterbi Modification - Technique II Algorithm :\", round(accuracy*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking set of all pairs where the tag that is assigned by us is not same as the tag assigned in the dataset\n",
    "incorrect_tagged_words = [[test_run_base[i-1],j] for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('military', 'ADJ'), (('command', 'VERB'), ('command', 'NOUN'))], [('ended', 'VERB'), (('up', 'ADV'), ('up', 'PRT'))]]\n"
     ]
    }
   ],
   "source": [
    "print(incorrect_tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "* The Time taken and accuracy of Viterbi Modification-Technique II - Rule Based Tagger:\n",
    "        * Time Taken  - Approximately 19 Seconds\n",
    "        * Accuracy    - 98.23 %\n",
    "* We got better Accuracy comapred to previous one\n",
    "* Below words are correctly POS Tagged by Viterbi Modification-Technique II than Technique I\n",
    "        * Sandinista - Viterbi Technique I - Modified|'VERB' - Viterbi Technique II|'NOUN'\n",
    "        * Eveready   - Viterbi Technique I - Modified|'VERB' - Viterbi Technique II|'NOUN'\n",
    "        * T*-252     - Vanilla Viterbi|'VERB' - Viterbi Technique I|'X'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm\n",
    "\n",
    "\n",
    "###  Considering above analysis - Viterbi Modified Technique II behaving well in the sample validation set(5 Sentences) compared to other techniques. Lets evaluate vanilla viterbi, Technique1 and Technique 2 on the entire validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tagged_words = [tup[0] for sent in validation_set for tup in sent]\n",
    "test_run_base = [tup for sent in validation_set for tup in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the entire Validation Set - Unmodified Viterbi POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmodified Vanilla Viterbi POS tagger - Time taken in seconds:  846.9\n",
      "Accuracy of Unmodified Vanilla Viterbi POS tagger : 90.89\n"
     ]
    }
   ],
   "source": [
    "# Tagging the test sentences - Calling Viterbi Technique 1 function\n",
    "# Using start time and end time to measure the timetaken\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "print(\"Unmodified Vanilla Viterbi POS tagger - Time taken in seconds: \", round(difference,1))\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "# Run this check array which takes set of all pairs from the tagged sequence and test_run_base\n",
    "# Only if the first and the second element have the same tag  then we note it down in the list\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "# Calculate Accuracy - Number of correct answers divided by Total number of tags\n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print(\"Accuracy of Unmodified Vanilla Viterbi POS tagger :\", round(accuracy*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the entire Validation Set - Viterbi Modification-Technique I "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi Modification-Technique I - Time taken in seconds:  965.0\n",
      "Accuracy of Viterbi Modification-Technique I : 94.46\n"
     ]
    }
   ],
   "source": [
    "# Tagging the test sentences - Calling Viterbi Technique 1 function\n",
    "# Using start time and end time to measure the timetaken\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi_Technique1_Modified(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "print(\"Viterbi Modification-Technique I - Time taken in seconds: \", round(difference,1))\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "# Run this check array which takes set of all pairs from the tagged sequence and test_run_base\n",
    "# Only if the first and the second element have the same tag  then we note it down in the list\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "# Calculate Accuracy - Number of correct answers divided by Total number of tags\n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print(\"Accuracy of Viterbi Modification-Technique I :\", round(accuracy*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the entire Validation Set - Viterbi Modification-Technique II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi Modification-Technique II - Time taken in seconds:  849.2\n",
      "Accuracy of Viterbi Modification-Technique II : 95.44\n"
     ]
    }
   ],
   "source": [
    "# Tagging the test sentences - Calling Viterbi Technique 1 function\n",
    "# Using start time and end time to measure the timetaken\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi_Technique2(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "print(\"Viterbi Modification-Technique II - Time taken in seconds: \", round(difference,1))\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "# Run this check array which takes set of all pairs from the tagged sequence and test_run_base\n",
    "# Only if the first and the second element have the same tag  then we note it down in the list\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "# Calculate Accuracy - Number of correct answers divided by Total number of tags\n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print(\"Accuracy of Viterbi Modification-Technique II :\", round(accuracy*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on entire Validation set Comparison:\n",
    "\n",
    "* The Time taken and accuracy of Unmodified Vanilla Viterbi POS tagger\n",
    "        * Time Taken  - Approximately 846.9 Seconds\n",
    "        * Accuracy    - 90.89 % \n",
    "        \n",
    "* The Time taken and accuracy of Viterbi Modification-Technique I - Modified\n",
    "        * Time Taken  - Approximately 965 Seconds\n",
    "        * Accuracy    - 94.46 % \n",
    "        \n",
    "* The Time taken and accuracy of Viterbi Modification-Technique II - Rule Based Tagger:\n",
    "        * Time Taken  - Approximately 849.2 Seconds\n",
    "        * Accuracy    - 95.44 % \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the given 'Test_sentences.txt' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening text file\n",
    "OpenTextFile=open('Test_sentences.txt')\n",
    "\n",
    "# Reading text file\n",
    "SampleSentences = OpenTextFile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitline() method is used to split the lines at line boundaries. \n",
    "#The function returns a list of lines in the string\n",
    "sample_test_sentences = SampleSentences.splitlines()\n",
    "\n",
    "#Closing the file\n",
    "OpenTextFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Android is a mobile operating system developed by Google.',\n",
       " 'Android has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013.',\n",
       " \"Google and Twitter made a deal in 2015 that gave Google access to Twitter's firehose.\",\n",
       " 'Twitter is an online news and social networking service on which users post and interact with messages known as tweets.',\n",
       " 'Before entering politics, Donald Trump was a domineering businessman and a television personality.',\n",
       " 'The 2018 FIFA World Cup is the 21st FIFA World Cup, an international football tournament contested once every four years.',\n",
       " 'This is the first World Cup to be held in Eastern Europe and the 11th time that it has been held in Europe.',\n",
       " 'Show me the cheapest round trips from Dallas to Atlanta',\n",
       " 'I would like to see flights from Denver to Philadelphia.',\n",
       " 'Show me the price of the flights leaving Atlanta at about 3 in the afternoon and arriving in San Francisco.',\n",
       " 'NASA invited social media users to experience the launch of ICESAT-2 Satellite.',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleTestSentences=sample_test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Android is a mobile operating system developed by Google.', 'Android has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013.', \"Google and Twitter made a deal in 2015 that gave Google access to Twitter's firehose.\", 'Twitter is an online news and social networking service on which users post and interact with messages known as tweets.', 'Before entering politics, Donald Trump was a domineering businessman and a television personality.', 'The 2018 FIFA World Cup is the 21st FIFA World Cup, an international football tournament contested once every four years.', 'This is the first World Cup to be held in Eastern Europe and the 11th time that it has been held in Europe.', 'Show me the cheapest round trips from Dallas to Atlanta', 'I would like to see flights from Denver to Philadelphia.', 'Show me the price of the flights leaving Atlanta at about 3 in the afternoon and arriving in San Francisco.', 'NASA invited social media users to experience the launch of ICESAT-2 Satellite.', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "print(sampleTestSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take the list of untagged words from the sentences given in the text file\n",
    "SampleWords_Test = [words for sentences in sampleTestSentences for words in sentences.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Android', 'is', 'a', 'mobile', 'operating', 'system', 'developed', 'by', 'Google.', 'Android', 'has', 'been', 'the', 'best-selling', 'OS', 'worldwide', 'on', 'smartphones', 'since', '2011', 'and', 'on', 'tablets', 'since', '2013.', 'Google', 'and', 'Twitter', 'made', 'a', 'deal', 'in', '2015', 'that', 'gave', 'Google', 'access', 'to', \"Twitter's\", 'firehose.', 'Twitter', 'is', 'an', 'online', 'news', 'and', 'social', 'networking', 'service', 'on', 'which', 'users', 'post', 'and', 'interact', 'with', 'messages', 'known', 'as', 'tweets.', 'Before', 'entering', 'politics,', 'Donald', 'Trump', 'was', 'a', 'domineering', 'businessman', 'and', 'a', 'television', 'personality.', 'The', '2018', 'FIFA', 'World', 'Cup', 'is', 'the', '21st', 'FIFA', 'World', 'Cup,', 'an', 'international', 'football', 'tournament', 'contested', 'once', 'every', 'four', 'years.', 'This', 'is', 'the', 'first', 'World', 'Cup', 'to', 'be', 'held', 'in', 'Eastern', 'Europe', 'and', 'the', '11th', 'time', 'that', 'it', 'has', 'been', 'held', 'in', 'Europe.', 'Show', 'me', 'the', 'cheapest', 'round', 'trips', 'from', 'Dallas', 'to', 'Atlanta', 'I', 'would', 'like', 'to', 'see', 'flights', 'from', 'Denver', 'to', 'Philadelphia.', 'Show', 'me', 'the', 'price', 'of', 'the', 'flights', 'leaving', 'Atlanta', 'at', 'about', '3', 'in', 'the', 'afternoon', 'and', 'arriving', 'in', 'San', 'Francisco.', 'NASA', 'invited', 'social', 'media', 'users', 'to', 'experience', 'the', 'launch', 'of', 'ICESAT-2', 'Satellite.']\n"
     ]
    }
   ],
   "source": [
    "print(SampleWords_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the entire 'Test_sentences.txt' file - Unmodified Vanilla Viterbi POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmodified Vanilla Viterbi POS tagger - Time taken in seconds:  28.8\n"
     ]
    }
   ],
   "source": [
    "# Tagging the test sentences - Calling Viterbi Vanilla function\n",
    "# Using start time and end time to measure the timetaken\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(SampleWords_Test)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Unmodified Vanilla Viterbi POS tagger - Time taken in seconds: \", round(difference,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'PRON'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google.', 'PRON'), ('Android', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', 'PRON'), ('worldwide', 'PRON'), ('on', 'ADP'), ('smartphones', 'PRON'), ('since', 'ADP'), ('2011', 'PRON'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013.', 'PRON'), ('Google', 'PRON'), ('and', 'CONJ'), ('Twitter', 'PRON'), ('made', 'VERB'), ('a', 'DET'), ('deal', 'NOUN'), ('in', 'ADP'), ('2015', 'PRON'), ('that', 'ADP'), ('gave', 'VERB'), ('Google', 'PRON'), ('access', 'NOUN'), ('to', 'PRT'), (\"Twitter's\", 'PRON'), ('firehose.', 'PRON'), ('Twitter', 'PRON'), ('is', 'VERB'), ('an', 'DET'), ('online', 'PRON'), ('news', 'NOUN'), ('and', 'CONJ'), ('social', 'ADJ'), ('networking', 'NOUN'), ('service', 'NOUN'), ('on', 'ADP'), ('which', 'DET'), ('users', 'NOUN'), ('post', 'NOUN'), ('and', 'CONJ'), ('interact', 'PRON'), ('with', 'ADP'), ('messages', 'PRON'), ('known', 'VERB'), ('as', 'ADP'), ('tweets.', 'PRON'), ('Before', 'ADP'), ('entering', 'VERB'), ('politics,', 'PRON'), ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('domineering', 'PRON'), ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), ('personality.', 'PRON'), ('The', 'DET'), ('2018', 'PRON'), ('FIFA', 'PRON'), ('World', 'NOUN'), ('Cup', 'PRON'), ('is', 'VERB'), ('the', 'DET'), ('21st', 'PRON'), ('FIFA', 'PRON'), ('World', 'NOUN'), ('Cup,', 'PRON'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), ('tournament', 'PRON'), ('contested', 'PRON'), ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years.', 'PRON'), ('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('first', 'ADJ'), ('World', 'NOUN'), ('Cup', 'PRON'), ('to', 'PRT'), ('be', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Eastern', 'NOUN'), ('Europe', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('11th', 'ADJ'), ('time', 'NOUN'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Europe.', 'PRON'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('cheapest', 'ADJ'), ('round', 'NOUN'), ('trips', 'PRON'), ('from', 'ADP'), ('Dallas', 'NOUN'), ('to', 'PRT'), ('Atlanta', 'NOUN'), ('I', 'PRON'), ('would', 'VERB'), ('like', 'ADP'), ('to', 'PRT'), ('see', 'VERB'), ('flights', 'NOUN'), ('from', 'ADP'), ('Denver', 'NOUN'), ('to', 'PRT'), ('Philadelphia.', 'PRON'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('price', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('flights', 'NOUN'), ('leaving', 'VERB'), ('Atlanta', 'NOUN'), ('at', 'ADP'), ('about', 'ADP'), ('3', 'NUM'), ('in', 'ADP'), ('the', 'DET'), ('afternoon', 'NOUN'), ('and', 'CONJ'), ('arriving', 'PRON'), ('in', 'ADP'), ('San', 'NOUN'), ('Francisco.', 'PRON'), ('NASA', 'PRON'), ('invited', 'PRON'), ('social', 'ADJ'), ('media', 'NOUN'), ('users', 'NOUN'), ('to', 'PRT'), ('experience', 'NOUN'), ('the', 'DET'), ('launch', 'NOUN'), ('of', 'ADP'), ('ICESAT-2', 'PRON'), ('Satellite.', 'PRON')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the entire Validation Set - Viterbi Modification-Technique I "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi Modification-Technique I - Time taken in seconds:  27.4\n"
     ]
    }
   ],
   "source": [
    "# Tagging the test sentences - Calling Viterbi Vanilla function\n",
    "# Using start time and end time to measure the timetaken\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi_Technique1_Modified(SampleWords_Test)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Viterbi Modification-Technique I - Time taken in seconds: \", round(difference,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google.', 'NOUN'), ('Android', 'NOUN'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', 'NOUN'), ('worldwide', 'NOUN'), ('on', 'ADP'), ('smartphones', 'NOUN'), ('since', 'ADP'), ('2011', 'NOUN'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013.', 'NOUN'), ('Google', 'NOUN'), ('and', 'CONJ'), ('Twitter', 'NOUN'), ('made', 'VERB'), ('a', 'DET'), ('deal', 'NOUN'), ('in', 'ADP'), ('2015', 'NOUN'), ('that', 'ADP'), ('gave', 'VERB'), ('Google', 'NOUN'), ('access', 'NOUN'), ('to', 'PRT'), (\"Twitter's\", 'NOUN'), ('firehose.', 'NOUN'), ('Twitter', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('online', 'NOUN'), ('news', 'NOUN'), ('and', 'CONJ'), ('social', 'ADJ'), ('networking', 'NOUN'), ('service', 'NOUN'), ('on', 'ADP'), ('which', 'DET'), ('users', 'NOUN'), ('post', 'NOUN'), ('and', 'CONJ'), ('interact', 'NOUN'), ('with', 'ADP'), ('messages', 'NOUN'), ('known', 'VERB'), ('as', 'ADP'), ('tweets.', 'NOUN'), ('Before', 'ADP'), ('entering', 'VERB'), ('politics,', 'NOUN'), ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('domineering', 'NOUN'), ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), ('personality.', 'NOUN'), ('The', 'DET'), ('2018', 'NOUN'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('21st', 'NOUN'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup,', 'NOUN'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), ('tournament', 'NOUN'), ('contested', 'NOUN'), ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years.', 'NOUN'), ('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('first', 'ADJ'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Eastern', 'NOUN'), ('Europe', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('11th', 'ADJ'), ('time', 'NOUN'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Europe.', 'NOUN'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('cheapest', 'ADJ'), ('round', 'NOUN'), ('trips', 'NOUN'), ('from', 'ADP'), ('Dallas', 'NOUN'), ('to', 'PRT'), ('Atlanta', 'NOUN'), ('I', 'PRON'), ('would', 'VERB'), ('like', 'ADP'), ('to', 'PRT'), ('see', 'VERB'), ('flights', 'NOUN'), ('from', 'ADP'), ('Denver', 'NOUN'), ('to', 'PRT'), ('Philadelphia.', 'NOUN'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('price', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('flights', 'NOUN'), ('leaving', 'VERB'), ('Atlanta', 'NOUN'), ('at', 'ADP'), ('about', 'ADP'), ('3', 'NUM'), ('in', 'ADP'), ('the', 'DET'), ('afternoon', 'NOUN'), ('and', 'CONJ'), ('arriving', 'NOUN'), ('in', 'ADP'), ('San', 'NOUN'), ('Francisco.', 'NOUN'), ('NASA', 'NOUN'), ('invited', 'NOUN'), ('social', 'ADJ'), ('media', 'NOUN'), ('users', 'NOUN'), ('to', 'PRT'), ('experience', 'NOUN'), ('the', 'DET'), ('launch', 'NOUN'), ('of', 'ADP'), ('ICESAT-2', 'NOUN'), ('Satellite.', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the entire Validation Set - Viterbi Modification-Technique II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi Modification-Technique II - Time taken in seconds:  26.9\n"
     ]
    }
   ],
   "source": [
    "# Tagging the test sentences - Calling Viterbi Vanilla function\n",
    "# Using start time and end time to measure the timetaken\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi_Technique2(SampleWords_Test)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Viterbi Modification-Technique II - Time taken in seconds: \", round(difference,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google.', 'NOUN'), ('Android', 'NOUN'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', 'NOUN'), ('worldwide', 'NOUN'), ('on', 'ADP'), ('smartphones', 'VERB'), ('since', 'ADP'), ('2011', 'NUM'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013.', 'NOUN'), ('Google', 'NOUN'), ('and', 'CONJ'), ('Twitter', 'NOUN'), ('made', 'VERB'), ('a', 'DET'), ('deal', 'NOUN'), ('in', 'ADP'), ('2015', 'NUM'), ('that', 'ADP'), ('gave', 'VERB'), ('Google', 'NOUN'), ('access', 'NOUN'), ('to', 'PRT'), (\"Twitter's\", 'NOUN'), ('firehose.', 'NOUN'), ('Twitter', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('online', 'NOUN'), ('news', 'NOUN'), ('and', 'CONJ'), ('social', 'ADJ'), ('networking', 'NOUN'), ('service', 'NOUN'), ('on', 'ADP'), ('which', 'DET'), ('users', 'NOUN'), ('post', 'NOUN'), ('and', 'CONJ'), ('interact', 'NOUN'), ('with', 'ADP'), ('messages', 'VERB'), ('known', 'VERB'), ('as', 'ADP'), ('tweets.', 'NOUN'), ('Before', 'ADP'), ('entering', 'VERB'), ('politics,', 'NOUN'), ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('domineering', 'VERB'), ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), ('personality.', 'NOUN'), ('The', 'DET'), ('2018', 'NUM'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('21st', 'NOUN'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup,', 'NOUN'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), ('tournament', 'NOUN'), ('contested', 'VERB'), ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years.', 'NOUN'), ('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('first', 'ADJ'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Eastern', 'NOUN'), ('Europe', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('11th', 'ADJ'), ('time', 'NOUN'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Europe.', 'NOUN'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('cheapest', 'ADJ'), ('round', 'NOUN'), ('trips', 'NOUN'), ('from', 'ADP'), ('Dallas', 'NOUN'), ('to', 'PRT'), ('Atlanta', 'NOUN'), ('I', 'PRON'), ('would', 'VERB'), ('like', 'ADP'), ('to', 'PRT'), ('see', 'VERB'), ('flights', 'NOUN'), ('from', 'ADP'), ('Denver', 'NOUN'), ('to', 'PRT'), ('Philadelphia.', 'NOUN'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('price', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('flights', 'NOUN'), ('leaving', 'VERB'), ('Atlanta', 'NOUN'), ('at', 'ADP'), ('about', 'ADP'), ('3', 'NUM'), ('in', 'ADP'), ('the', 'DET'), ('afternoon', 'NOUN'), ('and', 'CONJ'), ('arriving', 'VERB'), ('in', 'ADP'), ('San', 'NOUN'), ('Francisco.', 'NOUN'), ('NASA', 'NOUN'), ('invited', 'VERB'), ('social', 'ADJ'), ('media', 'NOUN'), ('users', 'NOUN'), ('to', 'PRT'), ('experience', 'NOUN'), ('the', 'DET'), ('launch', 'NOUN'), ('of', 'ADP'), ('ICESAT-2', 'NOUN'), ('Satellite.', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm\n",
    "\n",
    "The Time taken and accuracy of Unmodified Vanilla Viterbi POS tagger\n",
    "  * Accuracy of the Model                    - 90.89 %\n",
    "  * Time Taken on the Entire Vaidation Set   - Approximately 846.9 Seconds\n",
    "  * Time Taken on the Test_sentences file    - Approximately 28.8 Seconds\n",
    "\n",
    "The Time taken and accuracy of Viterbi Modification-Technique I - Modified\n",
    "  * Accuracy of the Model                    - 94.46 %\n",
    "  * Time Taken on the Entire Vaidation Set   - Approximately 965 Seconds\n",
    "  * Time Taken on the Test_sentences file    - Approximately 27.4 Seconds\n",
    "\n",
    "The Time taken and accuracy of Viterbi Modification-Technique II - Rule Based Tagger:\n",
    "  * Accuracy of the Model                    - 95.44 %\n",
    "  * Time Taken on the Entire Vaidation Set   - Approximately 849.2 Seconds\n",
    "  * Time Taken on the Test_sentences file    - Approximately 26.9 Seconds\n",
    "\n",
    "Considering this metrics, Viterbi technique II behaving well in the both validation set and test_sentences.txt file\n",
    "The words were incorrectly tagged in vanilla veterbi got corrected by modified Viterbi II Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications\n",
    "\n",
    "#### Below are some of the words were incorrectly tagged by original POS tagger and correct by veterbi modification algorithm\n",
    "\n",
    "##### Test_Sentence.txt file:\n",
    "      \n",
    "    * Android\t : 'ADJ'(Vanilla Veterbi)\t'NOUN'(Veterbi Technique I)\t'NOUN'(Veterbi Technique II)\n",
    "    * Google\t  : 'ADJ'(Vanilla Veterbi)\t'NOUN'(Veterbi Technique I)\t'NOUN'(Veterbi Technique II)\n",
    "    * Twitter\t : 'ADJ'(Vanilla Veterbi)\t'NOUN'(Veterbi Technique I)\t'NOUN'(Veterbi Technique II)\n",
    "    * messages    : 'ADJ'(Vanilla Veterbi)\t'NOUN'(Veterbi Technique I)\t'VERB'(Veterbi Technique II)\n",
    "    * known       : 'ADJ'(Vanilla Veterbi)\t'VERB'(Veterbi Technique I)\t'VERB'(Veterbi Technique II)\n",
    "    * domineering : 'ADJ'(Vanilla Veterbi)\t'NOUN'(Veterbi Technique I)\t'VERB'(Veterbi Technique II)\n",
    "    * contested   : 'ADJ'(Vanilla Veterbi)\t'NOUN'(Veterbi Technique I)\t'VERB'(Veterbi Technique II)\n",
    "    * arriving\t: 'ADJ'(Vanilla Veterbi)\t'NOUN'(Veterbi Technique I)\t'VERB'(Veterbi Technique II)\n",
    "    * invited     : 'ADJ'(Vanilla Veterbi)\t'NOUN'(Veterbi Technique I)\t'VERB'(Veterbi Technique II)\n",
    "\n",
    "* Most of the unknow words in test_sentences.txt file tagged as 'ADJ' in Vanilla Veterbi Technique\n",
    "* Android, Google, Twitter are correctly tagged as 'NOUN' in Veteri modified technique I and II\n",
    "* Messages, known, domineering, contested, arriving, invited are correctly tagges as 'VERB' only in Veterbi Modified Technique II\n",
    "\n",
    "##### Validation data set:\n",
    "\n",
    "    * Contra\t  : 'ADJ'(Vanilla Veterbi)\t'NOUN'(Veterbi Technique I)\t'NOUN'(Veterbi Technique II)\n",
    "    * Honduras\t: 'ADJ'(Vanilla Veterbi)\t'NOUN'(Veterbi Technique I)\t'NOUN'(Veterbi Technique II)\n",
    "    * rebel\t   : 'ADJ'(Vanilla Veterbi)\t'NOUN'(Veterbi Technique I)\t'NOUN'(Veterbi Technique II)\n",
    "    * Bucking\t : 'ADJ'(Vanilla Veterbi)\t'VERB'(Veterbi Technique I)\t'VERB'(Veterbi Technique II)\n",
    "    * drew\t    : 'ADJ'(Vanilla Veterbi)\t'VERB'(Veterbi Technique I)\t'VERB'(Veterbi Technique II)\n",
    "    * Eveready\t: 'ADJ'(Vanilla Veterbi)\t'VERB'(Veterbi Technique I)\t'NOUN'(Veterbi Technique II)\n",
    "    * complaining : 'ADJ'(Vanilla Veterbi)\t'VERB'(Veterbi Technique I)\t'VERB'(Veterbi Technique II)\n",
    "    * *T*-252\t : 'ADJ'(Vanilla Veterbi)\t'VERB'(Veterbi Technique I)\t'X'(Veterbi Technique II)\n",
    "    \n",
    "* Most of the unknow words in test_sentences.txt file tagged as 'ADJ' in Vanilla Veterbi Technique\n",
    "* Contra, Honduras, rebel, Eveready are correctly tagged as NOUN in modified Veterbi Algorithms\n",
    "* Bucking, complaining, drew are correctly tagged as VERB in modified veterbi algorithms\n",
    "* *T*-252 are correctly tagged as 'X' in Veterbi Technique II. It was incorrectly tagged by vannila veterbi and veterbi technique I"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
